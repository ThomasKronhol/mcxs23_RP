[
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Research question",
    "text": "Research question\nCan Bayesian VARs and alternative data help better estimate the future state of household final consumption expenditures in the US? Which combination of traditional and alternative data provides the highest accuracy in the latter indicator?"
  },
  {
    "objectID": "index.html#objective-and-motivation",
    "href": "index.html#objective-and-motivation",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Objective and Motivation",
    "text": "Objective and Motivation\nAs it drives around 50% of the US GDP, Personal Consumption Expenditures (PCE) is a leading indicator to gauge the economic health of a country. There is thus a high incentive to improve the accuracy of its predictions. This has encouraged researchers to investigate big data as alternative sources. For instance, Schimdt and Vosen (2009) use search query time-series provided by Google Trends to forecast consumption. Esteves (2009), Aprigliano, et al. (2019), Galbraith and Tkacz (2013), Carlsen and Storgaard (2010) analyze electronic payments to predict consumption as these can track a large percentage of spending activities. Ellingsen, et al. (2021) demonstrate that news media data capture information about consumption that hard economic indicators do not. Gil et al. (2018) investigate the potential of the Economic Policy Uncertainty index derived from news data and developed by Baker et al. (2016) to predict consumption.\nAccording to Professor Tomasz Wozniak from the University of Melbourne, forecasting with Bayesian VARs often leads to more precise forecasts than when using the frequentist approach to forecasting because of the effect of the priors. Despite the benefits of Bayesian VARs, there is nearly no research on the combination of Bayesian VARs and alternative data. Existing articles either investigate the use of Bayesian estimation models or of alternative data to forecast indicators but do not consider both together.\nThis paper will compare the forecasts of PCE in Australia from Bayesian VARs and several extensions applied on different sets of variables. These datasets will include both traditional macroeconomic variables computed by statistical offices and alternative data such as Google Trends. This research project contributes to the literature by studying PCE, an indicator that has to date received scant attention from the Bayesian VARs literature. Moreover, it proposes the first investigation of the combination of Bayesian VARs with alternative data to forecast PCE."
  },
  {
    "objectID": "index.html#preliminary-data-analysis",
    "href": "index.html#preliminary-data-analysis",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Preliminary data analysis",
    "text": "Preliminary data analysis\n\nAugmented Dickey-Fuller test for unit roots\n\nTable 1. ADF results for traditional log transformed variables\n\n\n\n\n  \n\n\n\n\n\nTable 2. ADF results for first difference of traditional variables\n\n\n\n\n  \n\n\n\n\n\nTable 3. ADF results for the second difference of Home price index"
  },
  {
    "objectID": "index.html#the-baseline-model",
    "href": "index.html#the-baseline-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "The Baseline Model",
    "text": "The Baseline Model\nThe model used for the forecasting experiment is a VAR(p) model:\n\\[\\begin{aligned}\n& y_t  =\\mu_0+A_1 y_{t-1}+\\cdots+A_p y_{t-p}+\\epsilon_t \\\\ & \\epsilon_t \\mid  Y_{t-1}  \\sim i i d \\mathcal{N}_N\\left(\\mathbf{0}_N, \\Sigma\\right)\n\\end{aligned}\\]\nWhere \\(N=11\\) and \\(y_t\\) is the vector of 11 variables:\n\\[\ny_t=\\left(\\begin{array}{cc}\\operatorname{pce}_t & =\\text {Real PCE }  \\\\\\operatorname{inc}_t & =\\text { Real household disposable income } \\\\\\text { cpi }_t & =\\text { Consumer price index } \\\\\\text { consum_sent}_t & =\\text { Consumer sentiment indicator }\\\\\\text { mortgage_rate}_t & =\\text { Mortgage rate }\\\\\\text { unemp_rate}_t & =\\text { Unemployment rate }\\\\\\text { home_price}_t & =\\text { Home price index } \\\\\\text{gt_dur_goods}_t & =\\text { Google Trends index for PCE of durable goods }\n\\\\\\text{gt_ndur_goods}_t & =\\text { Google Trends index for PCE of non durable goods }\n\\\\\\text{gt_services}_t & =\\text { Google Trends index for PCE of services }\n\\end{array}\\right)\n\\]\nThe model can also be written in matrix notation:\n\\[\\begin{aligned}\nY & =X A+E \\\\E \\mid X & \\sim \\mathcal{M N} _{T \\times N}\\left(\\mathbf{0}_{T \\times N}, \\Sigma, I_T\\right)\n\\end{aligned}\\]\nWhere \\(Y\\) is a \\(T\\times11\\) matrix, \\(X\\) is a \\(T\\times(1+(11\\times p))\\), \\(A\\) is a \\((1+(11\\times p))\\times 11\\) matrix that contains the relationships between the variables and \\(E\\) is a \\(T\\times11\\). T and p are yet to be determined during the continuation of this research project.\n\nThe Likelihood function\n\\[\n\\begin{aligned}\nY \\mid X,A,\\Sigma & \\sim \\mathcal{M N} _{T \\times N}\\left(XA, \\Sigma, I_T\\right)\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\nL(A, \\Sigma \\mid Y, X) & \\propto det(\\Sigma)^{-\\frac{T}{2}} \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X A)^{\\prime}(Y-X A)\\right]\\right\\} \\\\\n&= det (\\Sigma)^{-\\frac{T}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})\\right]\\right\\}\n\\end{aligned}\n\\]\n\n\nThe prior distributions\n\\[\n\\begin{aligned}\n&A \\mid \\Sigma \\sim  \\mathcal{M N} _{T \\times N}(\\underline{A}, \\Sigma, \\underline{V}) \\\\\n&\\Sigma \\sim \\mathcal{IW}_{N}(\\underline{S}, \\underline{\\nu})\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\np(A, \\Sigma) \\propto & \\operatorname{det}(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1} \\underline{S}\\right]\\right\\}\n\\end{aligned}\n\\] In the code, we set: A as V as S as Nu as\n\n\nThe posterior distribution\n\nThe Derivations of the posterior distribution\n\\[\n\\begin{aligned}\np(A,\\Sigma \\mid Y,X) \\propto L(A, \\Sigma \\mid Y,X) p(A,\\Sigma) = L(A, \\Sigma \\mid Y,X) p(A\\mid\\Sigma)p(\\Sigma)\n\\end{aligned}\n\\]\nLet’s focus on the kernel\n$$\n\\[\\begin{aligned}\np(A,\\Sigma \\mid Y,X) \\propto & det (\\Sigma)^{-\\frac{T}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})\\right]\\right\\} \\\\\n& \\times det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1} \\underline{S}\\right]\\right\\} \\\\\n\n& = \\operatorname{det}(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma ^ { - 1 }\n\n[\\color{green}{(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})+(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})}\\right\\} \\\\\n&\\color{green}{\\left.\\left.+(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})+\\underline{S}\\right]\\right]\\right\\}}\n\\end{aligned}\n$$\n\nWe can now complete the squares for the green part of the equation.\n\n\n\n```{=tex}\n\\begin{aligned}\n&\n\\color{blue}{(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})}\n\\color{green}{\n+(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})}\n\\color{red}{+(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})}\n\\color{black}{+\\underline{S}} \\\\\n\n& \\color{blue}{=A^\\prime X^\\prime XA -A^\\prime X^\\prime X \\widehat{A} - \\widehat{A}^\\prime X^\\prime XA + \\widehat{A}^\\prime X^\\prime X \\widehat{A}}\n\\color{green}{ +A^\\prime \\underline{V}^{-1}A - A^\\prime \\underline{V}^{-1} \\underline{A} - \\underline{A}^\\prime \\underline{V}^{-1} A + \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}}\n\\color{red}{+ Y^\\prime Y-Y^\\prime X \\widehat{A} - \\widehat{A}^\\prime X^\\prime Y + \\widehat{A}^\\prime X^\\prime X \\widehat{A}}\n\\color{black}{+ \\underline{S}}\\\\\n& \\color{blue}{=A^\\prime X^\\prime XA -Y^\\prime XA - \\widehat{A}^\\prime X^\\prime XA}\n\\color{green}{ +A^\\prime \\underline{V}^{-1}A - A^\\prime \\underline{V}^{-1} \\underline{A} - \\underline{A}^\\prime \\underline{V}^{-1} A}\n\\color{red}{+ Y^\\prime Y}\n\\color{black}{+\\underline{S}}\n\\color{green}{+ \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}}\\\\\n& = A^\\prime (X^\\prime X+ \\underline{V}^{-1})A -2A^\\prime (X^\\prime Y+\\underline{V}^{-1} \\underline{A}) + Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}\n\\end{aligned}\n```\n\n\n\nWe can set  $\\overline{V}^{-1}=X^\\prime X+ \\underline{V}^{-1}$\n\n\n$$\n\\begin{aligned}\n& = A^\\prime \\overline{V}^{-1}A -2A^\\prime \\overline{V}^{-1}\\overline{V} (X^\\prime Y+\\underline{V}^{-1} \\underline{A}) + Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}\n\\end{aligned}\n$$\n\nWe can set $\\overline{A}=\\overline{V}(X^\\prime Y+\\underline{V}^{-1}\\underline{A})$\n\n$$\n\\begin{aligned}\n& = A^\\prime \\overline{V}^{-1}A -2A^\\prime \\overline{V}^{-1}\\overline{A} \\pm \\overline{A}^\\prime \\overline{V}^{-1}\\overline{A} + Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}\\\\\n& =(A-\\overline{A})^\\prime \\overline{V}^{-1}(A-\\overline{A})-\\overline{A}^\\prime \\overline{V}^{-1}\\overline{A}+Y^\\prime Y+\\underline{S}+\\underline{A}^\\prime \\underline{V}^{-1} \\underline{A}\n\\end{aligned}\n$$\n\nLet's put the latter expression back in the $exp$.\n\n$$\n\\begin{aligned}\n\\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\overline{A})^{\\prime} \\overline{V}^{-1}(A-\\overline{A})\\right]\\right\\}\\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}\\overline{S}\\right]\\right\\}\n\\end{aligned}\\]\n$$\n\\[\n\\boxed{\n\\begin{array}{rcl}\n&p(A,\\Sigma \\mid Y,X)= p(A \\mid Y,X,\\Sigma)p(\\Sigma \\mid Y,X) \\\\\n&p(A \\mid Y,X,\\Sigma)=\\mathcal{M N}_{K\\times N}(\\overline{A},\\Sigma,\\overline{V})\\\\\n&p(\\Sigma \\mid Y,X)=\\mathcal{I W}_N (\\overline{S},\\overline{\\nu})\\\\\n& \\overline{V} = (X^\\prime X + \\underline{V}^{-1})^{-1}\\\\\n& \\overline{A}=\\overline{V}(X^\\prime Y+\\underline{V}^{-1}\\underline{A})\\\\\n& \\overline{\\nu}=T+\\underline{\\nu}\\\\\n& \\overline{S}=\\underline{S}+Y^\\prime Y+\\underline{A}^\\prime \\underline{V}^{-1}\\underline{A}-\\overline{A}^\\prime \\overline{V}^{-1}\\overline{A}\n\\end{array}\n}\n\\]\n\n\n\nThe Code for Bayesian VAR estimation\n\n# Bayesian estimation of the baseline model\n\n## Specify the setup\nN       = ncol(df.log)\np       = 4\nK       = 1+N*p\nS       = c(5000,100000)\nh       = 20\nset.seed(123456)\n\n## Create Y and X matrices\ny       = ts(df.log, start=c(2000,1), frequency=4)\nY       = ts(y[5:nrow(y),], start=c(2001,1), frequency=4)\nX       = matrix(1,nrow(Y),1)\nfor (i in 1:p){\n  X     = cbind(X,y[5:nrow(y)-i,])\n}\n\nT       = nrow(Y)\n## MLE\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T\n\n## Specify the priors (Minnesota prior)\nkappa.1           = 0.02^2\nkappa.2           = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\n\npriors = list(\n  A.prior     = A.prior,\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior     = diag(diag(Sigma.hat)),\n  nu.prior    = N+1 \n)\n\n## BVAR function\n\nBVAR = function(Y,X,priors,S){\n  \n  # normal-inverse Wishard posterior parameters\n  V.bar.inv   = t(X)%*%X + diag(1/diag(priors$V.prior))\n  V.bar       = solve(V.bar.inv)\n  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(priors$V.prior))%*%priors$A.prior)\n  nu.bar      = nrow(Y) + priors$nu.prior\n  S.bar       = priors$S.prior + t(Y)%*%Y + t(priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n  S.bar.inv   = solve(S.bar)\n  \n  #posterior draws\n  Sigma.posterior   = rWishart(sum(S), df=nu.bar, Sigma=S.bar.inv)\n  Sigma.posterior   = apply(Sigma.posterior,3,solve)\n  Sigma.posterior   = array(Sigma.posterior,c(N,N,sum(S)))\n  A.posterior       = array(rnorm(prod(c(dim(A.bar),sum(S)))),c(dim(A.bar),sum(S)))\n  L                 = t(chol(V.bar))\n  \n  for (s in 1:sum(S)){\n    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n  }\n  \n  posterior = list(\n    Sigma.posterior   = Sigma.posterior,\n    A.posterior       = A.posterior\n  )\n  return(posterior)\n}\n\n\n## Apply function BVAR\nposterior.draws = BVAR(Y=Y, X=X, priors=priors, S=S)\n\nThe output of the BVAR function applied on the baseline model is:\n\nround(apply(posterior.draws$A.posterior, 1:2, mean),3)\n\n        [,1]  [,2]   [,3]   [,4]   [,5]   [,6]   [,7]\n [1,] -0.004 0.007 -0.416 -0.008 -0.164  0.366 -0.007\n [2,]  1.000 0.000  0.000  0.000  0.000  0.000  0.000\n [3,]  0.000 1.000  0.000  0.000  0.000  0.000  0.000\n [4,]  0.000 0.000  1.007  0.000  0.002 -0.002  0.000\n [5,]  0.000 0.000  0.000  1.000  0.000  0.000  0.000\n [6,]  0.000 0.000  0.000  0.000  0.999  0.001  0.000\n [7,]  0.000 0.000 -0.002  0.001 -0.003  0.984  0.000\n [8,]  0.000 0.000  0.001  0.000  0.000  0.000  1.000\n [9,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[10,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[11,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[12,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[13,]  0.000 0.000  0.000  0.000  0.000  0.001  0.000\n[14,]  0.000 0.000  0.000  0.000 -0.001 -0.004  0.000\n[15,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[16,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[17,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[18,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[19,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[20,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[21,]  0.000 0.000  0.000  0.000  0.000 -0.001  0.000\n[22,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[23,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[24,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[25,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[26,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[27,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n[28,]  0.000 0.000  0.000  0.000  0.000 -0.001  0.000\n[29,]  0.000 0.000  0.000  0.000  0.000  0.000  0.000\n\n\n\nround(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)\n\n       [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]\n[1,]  0.000  0.000  0.001  0.000 0.000 -0.007  0.000\n[2,]  0.000  0.001 -0.001  0.000 0.000  0.007  0.000\n[3,]  0.001 -0.001  0.162 -0.003 0.020 -0.056  0.001\n[4,]  0.000  0.000 -0.003  0.003 0.000 -0.012  0.000\n[5,]  0.000  0.000  0.020  0.000 0.044  0.001  0.000\n[6,] -0.007  0.007 -0.056 -0.012 0.001  0.550 -0.001\n[7,]  0.000  0.000  0.001  0.000 0.000 -0.001  0.000"
  },
  {
    "objectID": "index.html#the-extended-model",
    "href": "index.html#the-extended-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "The extended model",
    "text": "The extended model\n\nThe prior distribution\n\\[\n\\begin{array}{rcl}\n&A \\mid \\Sigma,{\\color{red}\\kappa} \\sim  \\mathcal{M N} _{T \\times N}(\\underline{A}, \\Sigma, {\\color{red}\\kappa}\\underline{V}) \\\\\n&\\color{red}{\\kappa \\sim \\mathcal{IG2}(\\underline{s}_\\kappa,\\underline{\\nu}_\\kappa)}\\\\\n&\\Sigma \\sim \\mathcal{IW}_{N}(\\underline{S}, \\underline{\\nu})\n\\end{array}\n\\]\n\n\nThe posterior distribution\nIn this section, we will derive the the joint full-conditional posterior distribution of \\(A\\) and \\(\\Sigma\\) and the full-conditional posterior distribution of \\(\\kappa\\).\n\nThe derivations of the joint full-conditional posterior distribution of A and Sigma\n\\[\n\\begin{aligned}\np(A,\\Sigma \\mid Y,X,{\\color{red}\\kappa}) \\propto L(A, \\Sigma \\mid Y,X) p(A,\\Sigma) = L(A, \\Sigma \\mid Y,X) p(A\\mid\\Sigma,{\\color{red}\\kappa})p(\\Sigma)p({\\color{red}\\kappa})\n\\end{aligned} 4\n\\] Let’s focus on the kernel\n\\[\\begin{aligned}\np(A,\\Sigma \\mid Y,X,{\\color{red}{\\kappa}}) \\propto & det (\\Sigma)^{-\\frac{T}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})\\right]\\right\\} \\\\\n& \\times det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right]\\right\\} \\\\\n& \\times \\exp \\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1} \\underline{S}\\right]\\right\\}\\\\\n\n& = \\operatorname{det}(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\\\\n& \\times \\exp \\{-\\frac{1}{2} \\operatorname{tr}[\\Sigma^{-1} [(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})+(A-\\underline{A})^{\\prime} \\frac{1}{\\kappa} \\underline{V}^{-1}(A-\\underline{A}) \\\\\n& +(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})+\\underline{S}]]\\}\n\n\\end{aligned}\\]\n\\[\\begin{aligned}\n&\n(A-\\widehat{A})^{\\prime} X^{\\prime} X(A-\\widehat{A})\n+(A-\\underline{A})^{\\prime} {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1}(A-\\underline{A})\n+(Y-X \\widehat{A})^{\\prime}(Y-X \\widehat{A})\n+\\underline{S} \\\\\n\n& =A^\\prime X^\\prime XA -A^\\prime X^\\prime X \\widehat{A} - \\widehat{A}^\\prime X^\\prime XA + \\widehat{A}^\\prime X^\\prime X \\widehat{A}\n+A^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1}A - A^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A} - \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} A + \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A}\n+ Y^\\prime Y-Y^\\prime X \\widehat{A} - \\widehat{A}^\\prime X^\\prime Y + \\widehat{A}^\\prime X^\\prime X \\widehat{A}+ \\underline{S}\\\\\n\n& =A^\\prime X^\\prime XA -Y^\\prime XA - \\widehat{A}^\\prime X^\\prime XA\n+A^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1}A - A^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A} - \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} A\n+ Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A}\\\\\n& = A^\\prime (X^\\prime X+ {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1})A -2A^\\prime (X^\\prime Y+{\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A}) + Y^\\prime Y+ \\underline{S} + \\underline{A}^\\prime {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1} \\underline{A}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\boxed{\n\\begin{array}{rcl}\n&p(A,\\Sigma \\mid Y,X,{\\color{red}\\kappa})= \\mathcal{MNIW}_{K\\times N}(\\overline{A},\\overline{V},\\overline{S},\\overline{\\nu}) \\\\\n& \\overline{V} = (X^\\prime X + {\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1})^{-1}\\\\\n& \\overline{A}=\\overline{V}(X^\\prime Y+{\\color{red}{\\frac{1}{\\kappa}}}\\underline{V}^{-1}\\underline{A})\\\\\n& \\overline{\\nu}=T+\\underline{\\nu}\\\\\n& \\overline{S}=\\underline{S}+Y^\\prime Y+\\underline{A}^\\prime \\underline{V}^{-1}{\\color{red}{\\frac{1}{\\kappa}}}\\underline{A}-\\overline{A}^\\prime \\overline{V}^{-1}\\overline{A}\n\\end{array}\n}\n\\end{aligned}\\]\n\n\nThe Derivations of the full-conditional posterior distribution of \\(\\kappa\\)\n\\[\\begin{align}\np(\\kappa \\mid A, \\Sigma, Y,X) & =L(A,\\Sigma \\mid Y,X)p(\\kappa \\mid \\underline{s}_{\\kappa}, \\underline{\\nu}_{\\kappa})p(A,\\Sigma)\\\\\n& =L(A,\\Sigma \\mid Y,X)p(\\kappa \\mid \\underline{s}_{\\kappa}, \\underline{\\nu}_{\\kappa})p(A\\mid \\Sigma)p(\\Sigma)\\\\\n\n&= (\\kappa)^{-\\frac{\\underline{\\nu}+2}{2}}\\exp\\left\\{\\frac{-1}{2}\\frac{\\underline{s}_{\\kappa}}{\\kappa}\\right\\} \\\\\n& \\times det(\\kappa \\underline{V})^{-\\frac{N}{2}}\n\\exp\\left\\{-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\frac{1}{\\kappa}\\underline{V}^{-1}(A-\\underline{A})\\right]\\right\\} \\\\\n\n&= (\\kappa)^{-\\frac{\\underline{\\nu}+2}{2}}(\\kappa)^{-\\frac{KN}{2}}\\kappa ^{-\\frac{\\underline{\\nu}+KN+2}{2}}\\times \\exp\\left\\{\\frac{-1}{2} \\frac{1}{\\kappa}(\\underline{s}_{\\kappa}+\\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right])\\overline{s}_{\\kappa}\\right\\}\\\\\n\n&= \\kappa ^{-\\frac{\\overbrace{\\underline{\\nu}+KN}^{\\overline{\\nu}_{\\kappa}}+2}{2}}\\times \\exp\\left\\{\\frac{-1}{2} \\frac{1}{\\kappa}\\underbrace{(\\underline{s}_{\\kappa}+\\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right])}_{\\overline{s}_{\\kappa}}\\right\\}\n\n\\end{align}\\]\nWe obtain the following full-conditional posterior distribution of \\(\\kappa\\):\n\\[\n\\boxed{\n\\begin{array}{rcl}\n&\\kappa \\sim \\mathcal{IG2}(\\overline{s}_{\\kappa},\\overline{\\nu}_{\\kappa})\\\\\n& \\overline{s}_{\\kappa}=\\underline{s}_{\\kappa}+\\operatorname{tr}\\left[\\Sigma^{-1}(A-\\underline{A})^{\\prime} \\underline{V}^{-1}(A-\\underline{A})\\right]\\\\\n&\\overline{\\nu}_{\\kappa}=\\underline{\\nu}+KN\n\\end{array}\n}\n\\]\n\n\n\nThe code for Bayesian VAR estimation\nLet’s adapt the BVAR function to the extended model.\n\n#Extended model\n\n## Modify the priors\nkappa.1           = 1\nkappa.2           = 10\ninitial_kappa     = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\n\npriors = list(\n  A.prior            = A.prior,\n  V.prior            = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior            = diag(diag(Sigma.hat)),\n  nu.prior           = N+1,\n  s.kappa.prior      = 2,\n  nu.kappa.prior     = 4\n)\n\n## Modify BVAR function\n\nBVAR_extension = function(X,Y,priors,initial_kappa,S){\n  \n  A.posterior        = array(NA, dim = c(K,N,sum(S)))\n  Sigma.posterior    = array(NA,dim=c(N,N,sum(S)))\n  kappa.posterior    = matrix(NA, sum(S), 1) \n  kappa.posterior[1] = initial_kappa\n  \n  for (s in 1:sum(S)){\n    \n    # full-cond of joint posterior of A and Sigma\n    V.bar.inv   = t(X)%*%X + diag(1/diag(kappa.posterior[s]*priors$V.prior))\n    V.bar       = solve(V.bar.inv)\n    A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(kappa.posterior[s]*priors$V.prior))%*%priors$A.prior)\n    nu.bar      = nrow(Y) + priors$nu.prior\n    S.bar       = priors$S.prior + t(Y)%*%Y + t(priors$A.prior)%*%diag(1/diag(kappa.posterior[s]*priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv   = solve(S.bar)\n    \n    Sigma.posterior.dist   = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.draw             = apply(Sigma.posterior.dist,3,solve)\n    Sigma.posterior[,,s]   = Sigma.draw\n    A.posterior[,,s]            = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))\n    L                      = t(chol(V.bar))\n    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n    \n    #full conditional posterior of kappa\n    if (s!=sum(S)){\n    s.kappa.bar           = priors$s.kappa.prior + sum(diag(solve( Sigma.posterior[,,s])*t(A.posterior[,,s]-priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%(A.posterior[,,s]-priors$A.prior)))\n    nu.kappa.bar          = priors$nu.kappa.prior + (K*N)\n    kappa.draw            = s.kappa.bar/rchisq(1, df=nu.kappa.bar)\n    kappa.posterior[s+1]  = kappa.draw\n    }\n  }\n  \n  posterior.extension = list(\n    Sigma.posterior   = Sigma.posterior[,,S[1]+1:S[2]], #getting rid of first S[1] draws\n    A.posterior       = A.posterior[,,S[1]+1:S[2]],\n    kappa.posterior   = kappa.posterior[S[1]+1:S[2],1]\n  )\n  return(posterior.extension)\n}\n\n## Apply function BVAR_extension\nposterior.draws = BVAR_extension(Y=Y, X=X, priors=priors, initial_kappa=initial_kappa, S=S)\n\nThe output of the BVAR function applied on the baseline model is:\n\nround(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)\n\n       [,1]  [,2]   [,3]   [,4]  [,5]   [,6] [,7]\n[1,]  0.000 0.000  0.000  0.000 0.000 -0.005    0\n[2,]  0.000 0.000  0.000  0.000 0.000  0.002    0\n[3,]  0.000 0.000  0.093 -0.001 0.003 -0.011    0\n[4,]  0.000 0.000 -0.001  0.003 0.001 -0.007    0\n[5,]  0.000 0.000  0.003  0.001 0.032  0.002    0\n[6,] -0.005 0.002 -0.011 -0.007 0.002  0.298    0\n[7,]  0.000 0.000  0.000  0.000 0.000  0.000    0\n\nround(apply(posterior.draws$A.posterior, 1:2, mean),3)\n\n        [,1]   [,2]    [,3]   [,4]   [,5]    [,6]   [,7]\n [1,]  0.477  2.041 -28.906 -0.891 28.671   8.083  0.398\n [2,]  1.695 -1.474   9.537  1.914 -0.134 -76.629  0.048\n [3,] -0.109  0.443  -0.859 -0.174 -0.465   8.564 -0.018\n [4,]  0.000 -0.003   1.551 -0.037  0.099   0.010  0.000\n [5,]  0.025  0.025   0.606  0.931  0.528  -1.025  0.010\n [6,]  0.000 -0.005  -0.112 -0.004  0.936  -0.062 -0.002\n [7,]  0.017 -0.020   0.201  0.020  0.011  -0.234  0.001\n [8,]  0.071 -0.372   0.727  1.234 -1.364  -5.881  1.491\n [9,] -0.652  0.557  -0.867 -1.763 -0.664  57.860 -0.052\n[10,]  0.096  0.464   0.891  0.428 -0.316  -1.937  0.008\n[11,] -0.001  0.006  -0.783  0.059 -0.038  -0.001  0.000\n[12,] -0.039  0.003  -0.971 -0.270 -0.120   1.674  0.001\n[13,]  0.005 -0.005   0.157  0.001 -0.174  -0.348  0.000\n[14,] -0.014  0.010  -0.096 -0.026  0.000   1.107 -0.001\n[15,] -0.042  0.199   2.148 -1.283 -1.731   1.422 -0.291\n[16,] -0.078  0.288  -2.853  0.843  1.644   9.048 -0.036\n[17,]  0.036  0.045   0.616  0.097 -0.654  -0.366  0.024\n[18,]  0.002 -0.006   0.207 -0.039 -0.052   0.041 -0.001\n[19,]  0.034  0.020  -0.046  0.173 -0.255  -1.216 -0.003\n[20,] -0.006  0.007  -0.158  0.009  0.061   0.580  0.000\n[21,] -0.002  0.010  -0.092  0.001  0.000   0.144 -0.001\n[22,]  0.007  0.144  -0.235 -0.319  2.379   2.221 -0.145\n[23,] -0.017  0.396   0.154 -0.837 -1.534   5.468 -0.022\n[24,] -0.050 -0.008  -3.459 -0.292 -1.644  -2.092 -0.003\n[25,]  0.000  0.005  -0.015  0.016  0.004  -0.061  0.001\n[26,] -0.002  0.000   0.081  0.049 -0.017  -0.240 -0.005\n[27,]  0.001 -0.001   0.012 -0.005 -0.025  -0.214  0.000\n[28,]  0.002  0.000   0.046  0.000 -0.017  -0.161  0.000\n[29,] -0.013  0.085  -1.623  0.270  1.857   1.870 -0.046\n\nround(mean(posterior.draws$kappa.posterior),3)\n\n[1] 4032.023"
  },
  {
    "objectID": "index.html#forecasting-with-the-baseline-model",
    "href": "index.html#forecasting-with-the-baseline-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Forecasting with the baseline model",
    "text": "Forecasting with the baseline model\nTo build the point forecasts, we use numerical integration to sample from the joint predictive density in the following steps: \\01. We sample posterior draws from \\(p(A,\\Sigma \\mid Y,X)\\) by using the BVAR estimation sampler created above. \\02. We obtain $ { A{(s)},{(s)} }^S_{s=1}$ \\03. We sample draws from \\(\\hat{p}(Y_{t+h}\\mid Y_t)\\) by:\n\\[\\begin{align}\nY^{(s)}_{t+h} \\sim \\mathcal{N}_{hN}(Y_{t+h\\mid t}(A^{(s)}), Var[Y_{t+h \\mid t} \\mid A^{(s)}, \\Sigma ^{(s)}]))\n\\end{align}\\]\n\\04. We obtain \\(\\left\\{Y^{(s)}_{t+h}\\right\\}^{S}_{s=1}\\) \\05. Characterise of the predictive density using \\(\\left\\{Y^{(s)}_{t+h}\\right\\}^{S}_{s=1}\\)\n\n# simulate draws from the predictive density\nlibrary(mvtnorm)\nh = 20\nS = 50000\nY.h         = array(NA,c(h,N,S))\n\nfor (s in 1:S){\n  x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]\n  x.Ti        = x.Ti[p:1,]\n  for (i in 1:h){\n    x.T         = c(1,as.vector(t(x.Ti)))\n    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%posterior.draws$A.posterior[,,s], sigma=posterior.draws$Sigma.posterior[,,s])\n    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])\n  }\n}\n\n# plots of forecasts\nlibrary(plot3D)\nlibrary(MASS)\nlibrary(HDInterval)\n\npce.point.f    = apply(Y.h[,1,],1,mean) #one pce forecasts\npce.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)\npce.range      = range(y[,1],pce.interval.f)\n\nblue  = \"#05386B\"\nplum      =\"#BEBADA\"\nplum.rgb = col2rgb(\"thistle\")\nshade = rgb(plum.rgb[1],plum.rgb[2],plum.rgb[3],maxColorValue=255,alpha=100, names=\"thistle\")\n\n\n\npar(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)\nplot(1:(length(y[,1])+h),c(y[,1],pce.point.f), type=\"l\", ylim=pce.range, axes=FALSE, xlab=\"\", ylab=\"\", lwd=2, col=plum)\naxis(1,c(1,41,80,88, nrow(y),nrow(y)+h),c(\"2000\",\"2010\",\"2020\",\"2022\",\"\",\"\"), col=blue)\naxis(2,c(pce.range[1],mean(pce.range),pce.range[2]),c(\"\",\"HF Consumption Expenditures\",\"\"), col=blue)\nabline(v=92, col=\"gray42\")\ntext(x=91, y=12.7, srt=90, \"2022 Q4\")\nabline(v=96, col=\"gray42\")\ntext(x=95, y=12.7, srt=90, \"2023 Q4\")\npolygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[21:1]),\n        c(y[92,1],pce.interval.f[1,],pce.interval.f[2,20:1],y[92,1]),\n        col=shade, border=plum)\n\n\n\ndev.off()\n\nnull device \n          1 \n\n\nThe forecasts are built using the predictive density means and 90% highest density intervals. The above plot presents a sharp decline of personal consumption expenditures in the first quarters of 2023. A potential explanation could be that this drop is mainly driven by the decline in the data caused by the COVID-19 crisis. The personal consumption expenditures is then predicted to increase back in 2024."
  },
  {
    "objectID": "index.html#forecasting-with-the-extended-model",
    "href": "index.html#forecasting-with-the-extended-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Forecasting with the extended model",
    "text": "Forecasting with the extended model"
  },
  {
    "objectID": "index.html#set-up-of-the-model-proof",
    "href": "index.html#set-up-of-the-model-proof",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Set up of the model proof",
    "text": "Set up of the model proof\n\n### Specify the setup\np = 1\nN = 2\nK = 1+N*p\nS = c(5000,50000)\n\n### Generate RW data process\nrw.1    = cumsum(rnorm(1000,0,1))\nrw.2    = cumsum(rnorm(1000,0,1))\ny       = matrix(cbind(rw.1,rw.2),nrow=1000,ncol=N)\n\n### Create Y and X matrices\nY       = ts(y[2:nrow(y),])\nX       = matrix(1,nrow(Y),1)\nX       = cbind(X,y[1:nrow(y)-p,])"
  },
  {
    "objectID": "index.html#proof-of-baseline-model",
    "href": "index.html#proof-of-baseline-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Proof of baseline model",
    "text": "Proof of baseline model\n\n### MLE\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n### Specify the priors (Minnesota prior)\nkappa.1           = 0.02^2\nkappa.2           = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\n\npriors = list(\n  A.prior     = A.prior,\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior     = diag(diag(Sigma.hat)),\n  nu.prior    = N+1 \n)\n\n## Apply function BVAR\nposterior.draws = BVAR(Y=Y, X=X, priors=priors, S=S)\nround(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)\n\n       [,1]   [,2]\n[1,]  1.079 -0.062\n[2,] -0.062  1.051\n\nround(apply(posterior.draws$A.posterior, 1:2, mean),3)\n\n       [,1]  [,2]\n[1,] -0.109 0.127\n[2,]  0.985 0.001\n[3,] -0.001 0.996"
  },
  {
    "objectID": "index.html#proof-of-extended-model",
    "href": "index.html#proof-of-extended-model",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "Proof of extended model",
    "text": "Proof of extended model\n\n### Proof of extension model\n\n### MLE\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n### Modify the priors\nkappa.1           = 1\nkappa.2           = 10\ninitial_kappa     = 100\nA.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N+1),] = diag(N)\n\npriors = list(\n  A.prior            = A.prior,\n  V.prior            = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),\n  S.prior            = diag(diag(Sigma.hat)),\n  nu.prior           = N+1,\n  s.kappa.prior      = 2,\n  nu.kappa.prior     = 4\n)\n\n## Apply function BVAR_extension\nposterior.draws = BVAR_extension(Y=Y, X=X, priors=priors, initial_kappa=initial_kappa, S=S)\nround(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)\n\n       [,1]   [,2]\n[1,]  1.078 -0.061\n[2,] -0.061  1.051\n\nround(apply(posterior.draws$A.posterior, 1:2, mean),3)\n\n       [,1]  [,2]\n[1,] -0.116 0.128\n[2,]  0.984 0.001\n[3,] -0.001 0.996"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Forecasting Personal Consumption Expenditures using Bayesian VARs and Alternative Data",
    "section": "References",
    "text": "References\nAprigliano, V., Ardizzi, G., & Monteforte, L. (2019), “Using Payment System Data to Forecast Economic Activity,” International Journal of Central Banking, International Journal of Central Banking, vol. 15(4), pages 55-80, October.\nBaker, S. R., Bloom, N., & Davis, S. J. (2016). Measuring Economic Policy Uncertainty. The Quarterly Journal of Economics, 131(4), 1593–1636. https://doi.org/10.1093/qje/qjw024\nCarlsen, M. & Storgaard, P. E. (2010), “Dankort Payments as a Timely Indicator of Retail Sales in Denmark.” Danmarks Nationalbank Working Papers n°66.\nEllingsen, J., Larsen, V. H., & Thorsrud, L. A. (2021). News media versus FRED‐MD for macroeconomic forecasting. Journal of Applied Econometrics, 37(1), 63 – 81. https://doi.org/10.1002/jae.2859.\nEsteves, P. S. (2009), “Are ATM/POS Data Relevant When Nowcasting Private Consumption?”, Working Papers n°25, Banco de Portugal.\nGalbraith, J. W., & Tkacz, G. (2013). Nowcasting GDP: Electronic Payments, Data Vintages and the Timing of Data Releases. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2342228.\nGil, M., Perez, J. J., Sanchez Fuentes, A. J., & Urtasun, A. (2018). “Nowcasting Private Consumption: Traditional Indicators, Uncertainty Measures, Credit Cards and Some Internet Data”, Working Paper No. 1842, Banco de Espana.\nLazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). The Parable of Google Flu: Traps in Big Data Analysis. Science (New York, N.Y.), 343, 1203–1205. https://doi.org/10.1126/science.1248506\nSchmidt, T., & Vosen, S. (2009). Forecasting Private Consumption: Survey-Based Indicators vs. Google Trends. Journal of Forecasting, 30. https://doi.org/10.2139/ssrn.1514369"
  }
]